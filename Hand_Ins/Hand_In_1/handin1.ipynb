{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill in group number and member names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "GROUP = \"7\"\n",
    "NAME1 = \"Ali Bakly\"\n",
    "NAME2 = \"NA\"\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\DeclareMathOperator{\\prox}{prox}$\n",
    "$\\newcommand{\\R}{\\mathbb{R}}$\n",
    "$\\newcommand{\\Sym}{\\mathbb{S}}$\n",
    "$\\newcommand{\\norm}[1]{\\lVert{#1}\\rVert}$\n",
    "$\\DeclareMathOperator{\\sign}{sign}$\n",
    "$\\DeclareMathOperator{\\diag}{diag}$\n",
    "$\\DeclareMathOperator{\\relint}{relint}$\n",
    "$\\DeclareMathOperator{\\dom}{dom}$\n",
    "# Optimization for learning - FRTN50\n",
    "\n",
    "## Assignment 1\n",
    "\n",
    "The goal of this assignment is to become familiar with some of the steps involved in solving an optimization problem. In this assignment, you will form Fenchel dual problems, find gradients and/or proximal operators, and implement the proximal gradient method.\n",
    "\n",
    "__Problem__ The problem we will solve is the following constrained problem\n",
    "\n",
    "\\begin{align}\\label{eq:the_problem}\\tag{1}\n",
    "\t\\underset{x \\in S}{\\text{minimize}}\\; \\tfrac{1}{2}x^T Q x + q^Tx\n",
    "\\end{align}\n",
    "\n",
    "where $Q\\in\\mathbb{S}_{++}^{n}$, $q\\in\\mathbb{R}^{n}$ and $S\\subseteq\\mathbb{R}^{n}$ is a set defined by the points $a,b\\in\\mathbb{R}^{n}$, $a\\leq b$, such that \n",
    "\n",
    "\\begin{align*}\n",
    "\tS = \\{x \\in \\mathbb{R}^{n}: a \\leq x \\leq b \\}.\n",
    "\\end{align*}\n",
    "\n",
    "I.e., we are going to minimize a quadratic function over an $n$-dimensional box. Recall that, the vector inequality $a\\leq b$ means that \n",
    "\n",
    "\\begin{align*}\n",
    "\ta_{i} \\leq b_{i}\n",
    "\\end{align*}\n",
    "\n",
    "for each $i=1,\\ldots,n$. Define the function $f:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}$ such that\n",
    "\n",
    "\\begin{align*}\n",
    "\tf(x) = \\tfrac{1}{2}x^T Q x + q^Tx\n",
    "\\end{align*}\n",
    "\n",
    "for each $x\\in\\mathbb{R}^{n}$ and let $\\iota_{S}:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}\\cup\\{\\infty\\}$ denote the indicator function of the set $S$, i.e.,\n",
    "\n",
    "\\begin{align*}\n",
    "\t\\iota_{S}(x) =\n",
    "\t\\begin{cases}\n",
    "\t\t0 \t\t& \\text{if }x\\in S, \\\\\n",
    "\t\t\\infty \t& \\text{if }x\\in \\mathbb{R}^n \\setminus S.\n",
    "\t\\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "Problem \\eqref{eq:the_problem} can then be written as \n",
    "\n",
    "\\begin{align}\\label{eq:the_problem_mod}\\tag{2}\n",
    "\t\\underset{x \\in \\mathbb{R}^{n}}{\\text{minimize}}\\; f(x) + \\iota_{S}(x).\n",
    "\\end{align}\n",
    "\n",
    "__Solution method__ To solve optimization problem \\eqref{eq:the_problem_mod}, we will use the _proximal gradient method_. It solves problems of the form\n",
    "\n",
    "\\begin{align}\\label{eq:pgprob}\\tag{3}\n",
    "\t\\underset{x \\in \\mathbb{R}^{n}}{\\text{minimize}}\\; f(x) + g(x) \n",
    "\\end{align}\n",
    "\n",
    "where $f:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}$ is differentiable and $g:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}\\cup\\{\\infty\\}$ is proximable, i.e., $\\prox_{\\gamma g}$ can be cheaply computed. The proximal gradient method (with constant step-size) is given by:\n",
    "\n",
    "- Pick some arbitrary initial guess $x^0\\in\\R^{n}$ and step-size $\\gamma>0$.\n",
    "- For $k=0,1,2\\ldots$, let \n",
    "\\begin{align}\\label{eq:pg}\\tag{4}\n",
    "\t\t\t\tx^{k+1} = \\prox_{\\gamma g}\\left(x^k - \\gamma \\nabla f(x^k)\\right).\n",
    "\\end{align}\n",
    "- Stop when $x^k$ is deemed to have converged.\n",
    "\n",
    "In this assignment, we simply run the proximal gradient method a large fixed number of iterations and plot the norm of the fixed-point residual $\\norm{x^{k+1} - x^k}_{2}$ (also known as the step-length), of each step to make sure it converges to zero. Since the experiments are run on a computer, zero means smaller than machine precision, which usually is around $10^{-15}$.\n",
    "\n",
    "The step-size parameter $\\gamma$ in the \\eqref{eq:pg} will affect the convergence. It should be tuned to the problem or chosen based on properties of $f$ and $g$. In particular, suppose that $f$ and $g$ are proper, closed and convex. \n",
    "If $f$ is $\\beta$-smooth for some parameter $\\beta>0$, the maximal step-size to guarantee convergence is $\\gamma < \\frac{2}{\\beta}$.\n",
    "\n",
    "Below are the tasks that you need to solve. Keep this in mind:\n",
    "- The suggested exercises in the exercise compendium found on the Canvas course page, up until and including the chapter \"Proximal gradient method - basics\", is relevant for this assignment. \n",
    "- Carefully motivate every step in your calculations.\n",
    "- Use __figures__ and __tables__ to motivate your answers.\n",
    "- Figures must have appropriately labeled axes and must be referenced in the main text.\n",
    "- Your code should be written in a quite general manner, i.e., if a question is slightly modified, it should only require slight modifications in your code as well. \n",
    "- Comment your code well. \n",
    "- Make sure you plot in such a way that small quantities (e.g., $\\norm{x^{k+1} - x^k}_{2}$) are visible. In particular, use log-linear plots, where the quantity that should go to $0$ is on the $y$-axis using logarithmic scale, and the iteration number $k$ on the $x$-axis using linear scale.\n",
    "- What you need to submit to Canvas:\n",
    "    - This jupyter notebook containing your solutions.\n",
    "    - An exported pdf version of the jupyter notebook. (One way to do this is to print the notebook in your web browser, and then save as pdf.)\n",
    "\n",
    "This table shows which lectures are needed in order to solve task 1-7:\n",
    "    \n",
    "| Task | Lectures |\n",
    "| ---- | -------- |\n",
    "| 1    | 2, 3     |\n",
    "| 2    | 4        |\n",
    "| 3    | 3, 4     |\n",
    "| 4    | 2, 4     |\n",
    "| 5    | 3, 4     |\n",
    "| 6    | 5        |\n",
    "| 7    | 4        |\n",
    "\n",
    "Task 8-10 can be solved after you have solved task 1-7. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\DeclareMathOperator{\\prox}{prox}$\n",
    "$\\newcommand{\\R}{\\mathbb{R}}$\n",
    "$\\newcommand{\\Sym}{\\mathbb{S}}$\n",
    "$\\newcommand{\\norm}[1]{\\lVert{#1}\\rVert}$\n",
    "$\\DeclareMathOperator{\\sign}{sign}$\n",
    "$\\DeclareMathOperator{\\diag}{diag}$\n",
    "$\\DeclareMathOperator{\\relint}{relint}$\n",
    "$\\DeclareMathOperator{\\dom}{dom}$\n",
    "---\n",
    "### Task 1:\n",
    "\n",
    "Show that $f$ and $\\iota_{S}$ in (2) are convex and show that constraint qualification (CQ) holds. You are allowed to assume that $\\relint S \\neq \\emptyset$. Note that $f$ and $\\iota_{S}$ also are closed, but you do not need to prove this.\n",
    "\n",
    "__Solution:__ \n",
    "\n",
    "Let us first show that $f$ is convex. We remind ourselves about the second order condition for convexity:\n",
    "* $f$ is convex $\\iff \\nabla^2 f(x) \\succeq 0 \\quad \\forall x \\in \\mathbb{R}^n,$ i.e the hessian of $f$ is semi-positive definite.\n",
    "  \n",
    "Recall that\n",
    "$$f(x) = \\frac{1}{2}x^T Q x + q^Tx \\implies \\nabla f = Qx+q \\implies \\nabla^2 f = Q.$$\n",
    "By assumption $Q\\in\\mathbb{S}_{++}^{n}$ so $\\nabla^2 f(x) \\succ 0$ and $\\nabla^2 f(x) \\succ 0$, fullfilling the second order condition for convexity.\n",
    "\n",
    "Now let us show that $\\iota_{S}$ is convex. As seen in class $\\iota_{S} \\;\\; \\text{convex} \\iff S \\;\\; \\text{convex}$, so it suffices to show that $S$ is convex. Take any $x,y \\in S$ and $\\theta \\in [0,1]$. Then we must show that $a \\leq \\theta x + (\\theta -1 )y \\leq b$. Since $a \\leq x \\leq b$ and $a \\leq y \\leq b$ we have \n",
    "\\begin{align*}\n",
    "\\theta a + (1- \\theta)&a \\leq \\theta x + (1- \\theta)y \\leq \\theta b + (1- \\theta)b \\\\\n",
    "&\\;\\;\\;\\;\\;\\;\\;\\; \\iff \\\\\n",
    "&a \\leq \\theta x + (1 - \\theta)y \\leq b,\n",
    "\\end{align*}tand thus we have $\\theta x + (\\theta -1 )y \\in S$.\n",
    "\n",
    "Finally let us show that the constraint qualification (CQ) holds, i.e that $$\\relint{\\dom f} \\cap \\relint{\\dom g} \\neq \\emptyset$$ Note that $f$ is a quadratic function defined everywhere in $\\mathbb{R}^n$, therefore its domain is $\\mathbb{R}^n$. Since $\\text{dom}(f) = \\mathbb{R}^n$, its relative interior is $\\relint{\\dom f} = \\relint{\\mathbb{R}^n} = \\mathbb{R}^n$. The domain of $g$ (i.e., where $g$ is finite) is $S$, thus $\\relint{\\dom g} = \\relint{S} \\subset \\mathbb{R}^n$. Therefor we have $$\\relint{\\dom f} \\cap \\relint{\\dom g} = \\mathbb{R}^n \\cap \\relint{S} = \\relint{S} \\neq \\emptyset,$$ where $\\relint{S} \\neq \\emptyset$ by assumption.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\DeclareMathOperator{\\prox}{prox}$\n",
    "$\\newcommand{\\R}{\\mathbb{R}}$\n",
    "$\\newcommand{\\Sym}{\\mathbb{S}}$\n",
    "$\\newcommand{\\norm}[1]{\\lVert{#1}\\rVert}$\n",
    "$\\DeclareMathOperator{\\sign}{sign}$\n",
    "$\\DeclareMathOperator{\\diag}{diag}$\n",
    "$\\DeclareMathOperator{\\relint}{relint}$\n",
    "$\\DeclareMathOperator{\\dom}{dom}$\n",
    "---\n",
    "### Task 2:\n",
    "\n",
    "Compute the conjugate functions $f^\\ast$ and $\\iota_{S}^\\ast$. The final expressions are not allowed to be given implicitly via optimization problems. E.g., projection formulas must be solved explicitly.\n",
    "\n",
    "__Solution:__ \n",
    "\n",
    "Let us start with $f^\\ast$. The gradient satisfies $\\nabla f(x) = Qx + q$\n",
    "\n",
    "Recall that $$f^*(s) = \\sup_{x \\in \\mathbb{R}^n} \\left( s^T x - \\frac{1}{2} x^T Q x - q^T x \\right),$$ and let $g(s,x) = s^T x - \\frac{1}{2} x^T Q x - q^T x$. Fermat's rule for $f^*(s)$ yields:\n",
    "\n",
    "\\begin{align}\n",
    "0 = s - Qx - q \\quad \\iff \\quad x = Q^{-1}(s - q)\n",
    "\\end{align}\n",
    "\n",
    "Note that in general one would have $0 \\in \\partial g$, but since $g$ is differetiable with $\\nabla_x g= s - Qx - q$ we have $\\partial g= \\{\\nabla_x g\\} = \\{s - Qx - q\\}$. Since this is a singleton set we have equality. Finally by substituting,\n",
    "\n",
    "\\begin{align}\n",
    "f^*(s) &= s^T Q^{-1}(s - q) - \\frac{1}{2} (s - q)^T Q^{-1} Q Q^{-1} (s - q) - q^T Q^{-1}(s - q) \\\\\n",
    "&= \\frac{1}{2}(s - q)^T Q^{-1}(s - q).\n",
    "\\end{align}\n",
    "\n",
    "Let us now treat $\\iota_{S}^\\ast$. Again, $$\\iota_{S}^\\ast(s) = \\sup_{x \\in \\mathbb{R}^n} \\left( s^T x - \\iota_{S}(x) \\right).$$ Note that for $x \\in \\mathbb{R}\\smallsetminus S$ results on $\\iota_{S}(x) = \\infty$ and $s^T x - \\iota_{S}(x) = -\\infty$. This means that the supremum is never attained at $x \\in \\mathbb{R}\\smallsetminus S$, letting us write: $$\\iota_{S}^\\ast(s) = \\sup_{x \\in S} \\left( s^T x - \\iota_{S}(x) \\right).$$ But, if $x \\in S$ then $\\iota_{S}(x) = 0$ and $$\\iota_{S}^\\ast(s) = \\sup_{x \\in S} \\left( s^T x \\right)= \\sup_{(x_1,\\ldots, x_n) \\in S} \\left(\\sum_{i=1}^n s_ix_i\\right) = \\sum_{i=1}^n \\sup_{a_i \\leq x_i \\leq b_i}s_ix_i.$$ It is also simple to see that $$\\sup_{a_i \\leq x_i \\leq b_i}s_ix_i = \n",
    "\\begin{cases} \n",
    "s_i a_i & \\text{if } s_i \\geq 0, \\\\\n",
    "s_i b_i & \\text{if } s_i < 0.\n",
    "\\end{cases}$$\n",
    "Finally, we have: $$\\iota_{S}^\\ast(s)=\\sum_{i=1}^{n}\\begin{cases} \n",
    "s_i a_i & \\text{if } s_i \\geq 0, \\\\\n",
    "s_i b_i & \\text{if } s_i < 0.\n",
    "\\end{cases}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\DeclareMathOperator{\\prox}{prox}$\n",
    "$\\newcommand{\\R}{\\mathbb{R}}$\n",
    "$\\newcommand{\\Sym}{\\mathbb{S}}$\n",
    "$\\newcommand{\\norm}[1]{\\lVert{#1}\\rVert}$\n",
    "$\\DeclareMathOperator{\\sign}{sign}$\n",
    "$\\DeclareMathOperator{\\diag}{diag}$\n",
    "$\\DeclareMathOperator{\\relint}{relint}$\n",
    "$\\DeclareMathOperator{\\dom}{dom}$\n",
    "---\n",
    "### Task 3:\n",
    "\n",
    "Write down a Fenchel dual problem to (2). Show that constraint qualification for the dual problem (CQ-D) holds.\n",
    "\n",
    "_Attention/hint:_ Keep track of your minus signs.\n",
    "\n",
    "__Solution:__ \n",
    "\n",
    "We are considering the primal problem\n",
    "$$\\underset{x \\in \\mathbb{R}^{n}}{\\text{minimize}}\\; f(x) + \\iota_{S}(Lx),$$ with $L = I$ (the identity). Letting $\\mu \\in \\partial\\iota_{S}(Lx)$, the dual problem is \n",
    "$$\\underset{\\mu \\in \\mathbb{R}^{n}}{\\text{minimize}}\\; f^\\ast(-L\\mu) + \\iota^\\ast_{S}(\\mu) = \\underset{\\mu \\in \\mathbb{R}^{n}}{\\text{minimize}}\\; f^\\ast(-\\mu) + \\iota^\\ast_{S}(\\mu) .$$ \n",
    "To check the constraint qualification we should verify the following:\n",
    "$$ \\relint{\\dom f^\\ast \\circ -I} \\cap \\relint{\\dom  \\iota^\\ast_{S}} \\neq \\emptyset.$$\n",
    "This clearly holds since \n",
    "$$\\relint{\\dom f^\\ast \\circ -I} = \\relint{\\dom f^\\ast} =  \\relint{\\mathbb{R^n}} = \\mathbb{R^n},$$\n",
    "and  \n",
    "$$\\relint{\\dom  \\iota^\\ast_{S}}=\\relint{\\mathbb{R}^n}= \\mathbb{R}^n,$$\n",
    "which certainly leads to\n",
    "$$ \\relint{\\dom f^\\ast \\circ -I} \\cap \\relint{\\dom  \\iota^\\ast_{S}} \\neq \\emptyset$$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\DeclareMathOperator{\\prox}{prox}$\n",
    "$\\newcommand{\\R}{\\mathbb{R}}$\n",
    "$\\newcommand{\\Sym}{\\mathbb{S}}$\n",
    "$\\newcommand{\\norm}[1]{\\lVert{#1}\\rVert}$\n",
    "$\\DeclareMathOperator{\\sign}{sign}$\n",
    "$\\DeclareMathOperator{\\diag}{diag}$\n",
    "$\\DeclareMathOperator{\\relint}{relint}$\n",
    "$\\DeclareMathOperator{\\dom}{dom}$\n",
    "---\n",
    "### Task 4:\n",
    "\n",
    "Show that $f$ and $f^*$ are $\\beta$-, and $\\beta^*$-smooth, respectively. Find expressions for the smallest such parameters $\\beta$ and $\\beta^*$.\n",
    "\n",
    "_Hint:_ Later when calculating the smoothness parameters in Pyhton, make sure to read the documentation carefully so that you use the correct function.\n",
    "\n",
    "__Solution:__ \n",
    "\n",
    "For $f$ we know that $f$ is $\\beta$-smooth if and only if\n",
    "$$0\\preceq\\nabla^2 f(x) \\preceq\\beta I,$$\n",
    "using that $f$ is convex (otherwise we need to replace $0$ with $-\\beta I$). This means that we should show that $\\beta I$ minus the hessian $\\nabla^2 f(x) = Q$  is positive semi-definite. Note that $0\\preceq\\nabla^2 f(x)$ is true by assumption of $Q\\in\\mathbb{S}_{++}$. Now, let the eigenvalue(s) of $Q$ be denoted by $\\lambda$, i.e $Qx = \\lambda x$. Note that $(\\beta I - Q)^\\top = \\beta I^\\top - Q^\\top = \\beta I - Q$, meaning that  $(\\beta I - Q)$ is symmetric (and hermitian). Recall that a hermitian matrix is positive semi-definiten if and only if all of its eigenvalues are non-negative. Consider the following:\n",
    "$$\n",
    "(\\beta I - Q)x = \\beta x - Qx = \\beta x - \\lambda x = (\\beta - \\lambda)x,\n",
    "$$\n",
    "which implies that $\\beta I - Q$ has the eigenvalue(s) $\\beta - \\lambda$. For positive semi-definiteness we therefore need $\\beta - \\lambda_i \\geq 0 \\iff \\beta \\geq \\lambda_i$ for all eigenvalues $\\lambda_i$. The smallest $\\beta$ such that this is upheld is \n",
    "$$\\beta = \\underset{i}{\\text{max}}\\; \\lambda_i = \\lambda_{\\text{max}}$$\n",
    "\n",
    "For $f^\\ast$ we can do the corresponding analysis but with $\\nabla^2 f(x) = Q^{-1}$, and show $$0\\preceq Q^{-1} \\preceq\\beta^\\ast I.$$ First we consider $0\\preceq Q^{-1}.$ Since $(Q^{-1})^\\top = (Q^\\top)^{-1} = Q^{-1}$, $Q^{-1}$ is symmetric and if $Q$ has eigenvalue $\\lambda>0$ then $Q^{-1}$ has the eigenvalue $1/\\lambda > 0$, which implies $Q^{-1}$ is positive definite and therefore also positive semi definite. Now, lets consider $Q^{-1} \\preceq\\beta^\\ast I$. The same analysis as before would lead to $\\beta^\\ast \\geq 1/\\lambda_i$ for all eigenvalues $\\lambda_i$. The smallest $\\beta^\\ast$ such that this is upheld is $$\\beta^\\ast = \\underset{i}{\\text{min}}\\; 1/\\lambda_i = 1/\\lambda_{\\text{min}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\DeclareMathOperator{\\prox}{prox}$\n",
    "$\\newcommand{\\R}{\\mathbb{R}}$\n",
    "$\\newcommand{\\Sym}{\\mathbb{S}}$\n",
    "$\\newcommand{\\norm}[1]{\\lVert{#1}\\rVert}$\n",
    "$\\DeclareMathOperator{\\sign}{sign}$\n",
    "$\\DeclareMathOperator{\\diag}{diag}$\n",
    "$\\DeclareMathOperator{\\relint}{relint}$\n",
    "$\\DeclareMathOperator{\\dom}{dom}$\n",
    "$\\DeclareMathOperator*{\\argmin}{argmin}$\n",
    "\n",
    "---\n",
    "### Task 5:\n",
    "\n",
    "Compute $\\nabla f$, $\\nabla f^\\ast$, $\\prox_{\\gamma\\iota_{S}}$ and $\\prox_{\\gamma\\iota_{S}^\\ast}$. The final expressions are not allowed to be given implicitly via optimization problems. E.g., projection formulas must be solved explicitly.\n",
    "\n",
    "\n",
    "__Solution:__ \n",
    "The gradients are given as the folllowing:\n",
    "$$f(x)=\\frac{1}{2}x^TQx+q^Tx \\implies \\nabla f(x)=Qx+q$$\n",
    "$$f^\\ast(s)=\\frac{1}{2}(s-q)^TQ^{-1}(s-q) \\implies \\nabla f^\\ast (s)=Q^{-1}(s-q)$$\n",
    "\n",
    "For $\\prox_{\\gamma\\iota_{S}}$ we use the definition:\n",
    "\n",
    "$$\\prox_{\\gamma\\iota_{S}}(z) = \\argmin_{x \\in \\mathbb{R}^n} \\left( \\frac{1}{2\\gamma} \\| x - z \\|_2^2 + \\iota_S(x) \\right)= \\argmin_{x \\in S} \\left( \\frac{1}{2\\gamma} \\| x - z \\|_2^2 \\right) = \\argmin_{x \\in S} \\left( \\| x - z \\|_2 \\right). $$\n",
    "Note that this is precisely the projection of the point $z$ into the set $S = \\{x \\in \\mathbb{R}^{n}: a \\leq x \\leq b \\}.$ To minimize $\\| x - z \\|_2$ one need to choose $x$ such that each element $|x_i-z_i|$ is as small as possible. This is accomplished with\n",
    "$$x_i =\n",
    "\\begin{cases} \n",
    "a_i & \\text{if } z_i < a_i, \\\\\n",
    "z_i & \\text{if } a_i \\leq z_i \\leq b_i,\\\\\n",
    "b_i & \\text{if } z_i > b_i,\n",
    "\\end{cases}\n",
    "$$\n",
    "and $ \\prox_{\\gamma\\iota_{S}}(z) = [x_1,..., x_n]^\\top$, where each $x_i$ are selected as above. One could also have utilized Fermats rule to reach this result.\n",
    "\n",
    "For $\\prox_{\\gamma\\iota_{S}^\\ast}$ we use the Moreau decomposition:\n",
    "$$\n",
    "z = \\prox_{\\gamma\\iota_{S}}(z)+\\prox_{(\\gamma \\iota_{S})^\\ast}(z)=\\prox_{\\gamma \\iota_{S}}(z)+\\gamma\\prox_{\\gamma^{-1}\\iota_{S}^\\ast}(\\gamma^{-1}z),\n",
    "$$\n",
    "but instead of using $\\iota_{S}$ in the decomposition above we use $\\iota_{S}^\\ast$:\n",
    "$$\n",
    "z = \\prox_{\\gamma\\iota_{S}^\\ast}(z)+\\prox_{(\\gamma \\iota_{S}^\\ast)^\\ast}(z)=\\prox_{\\gamma \\iota_{S}^\\ast}(z)+\\gamma\\prox_{\\gamma^{-1}\\iota_{S}^{\\ast \\ast}}(\\gamma^{-1}z) = \\prox_{\\gamma \\iota_{S}^\\ast}(z)+\\gamma\\prox_{\\gamma^{-1}\\iota_{S}}(\\gamma^{-1}z),\n",
    "$$\n",
    "where we in the last equality used the fact that $\\iota_{S} = \\iota_{S}^{\\ast \\ast}$ (implied by closed an convex). Meaning we have\n",
    "$$\n",
    "\\prox_{\\gamma \\iota_{S}^\\ast}(z) = z - \\gamma\\prox_{\\gamma^{-1}\\iota_{S}}(\\gamma^{-1}z).\n",
    "$$\n",
    "Thus, \n",
    "$$\n",
    "\\prox_{\\gamma \\iota_{S}^\\ast}(z) = [x_1,..., x_n]^\\top,\n",
    "$$\n",
    "where $x_i$ is now instead given by\n",
    "$$\n",
    "x_i=\\begin{cases} \n",
    "z_i-\\gamma a_i & \\text{if } \\gamma^{-1}z_i < a_i, \\\\\n",
    "0 & \\text{if } a_i \\leq \\gamma^{-1}z_i \\leq b_i,\\\\\n",
    "z_i - \\gamma b_i & \\text{if } \\gamma^{-1}z_i > b_i.\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\DeclareMathOperator{\\prox}{prox}$\n",
    "$\\newcommand{\\R}{\\mathbb{R}}$\n",
    "$\\newcommand{\\Sym}{\\mathbb{S}}$\n",
    "$\\newcommand{\\norm}[1]{\\lVert{#1}\\rVert}$\n",
    "$\\DeclareMathOperator{\\sign}{sign}$\n",
    "$\\DeclareMathOperator{\\diag}{diag}$\n",
    "$\\DeclareMathOperator{\\relint}{relint}$\n",
    "$\\DeclareMathOperator{\\dom}{dom}$\n",
    "---\n",
    "### Task 6:\n",
    "\n",
    "Based on your results above, write explicitly out the proximal gradient update rule (4) for both the primal and the dual problem. Use $x$ as the primal variable and $\\mu$ as the dual variable.\n",
    "\n",
    "_Attention/hint:_ Keep track of your minus signs.\n",
    "\n",
    "__Solution:__ \n",
    "\n",
    "We recall that for problems of the form\n",
    "\\begin{align}\n",
    "\t\\underset{x \\in \\mathbb{R}^{n}}{\\text{minimize}}\\; f(x) + g(x), \n",
    "\\end{align}\n",
    "\n",
    "The proximal gradient update (with constant step-size) is given by:\n",
    "\n",
    "\\begin{align}\n",
    "\t\t\t\tx^{k+1} = \\prox_{\\gamma g}\\left(x^k - \\gamma \\nabla f(x^k)\\right).\n",
    "\\end{align}\n",
    "In our case we have $g = \\iota_S$ and $ \\nabla f(x^k) =Qx^k + q$, meaning\n",
    "\\begin{align}\n",
    "\t\t\t\tx^{k+1} = \\prox_{\\gamma \\iota_S}\\left(x^k - \\gamma (Qx^k + q)\\right).\n",
    "\\end{align}\n",
    "Letting $v^k = x^k - \\gamma (Qx^k + q)$, we have\n",
    "\\begin{align}\n",
    "\t\t\t\tx^{k+1} = \\prox_{\\gamma \\iota_S}\\left(v^k\\right).\n",
    "\\end{align}\n",
    "We know from the previous task that \n",
    "$$ x^{k+1} = \\prox_{\\gamma\\iota_{S}}(v^k) = [x_1^k,..., x_n^k]^\\top\n",
    "$$ \n",
    "where\n",
    "$$x_i^k =\n",
    "\\begin{cases} \n",
    "a_i & \\text{if } v_i^k < a_i, \\\\\n",
    "v_i^k & \\text{if } a_i \\leq v_i^k \\leq b_i,\\\\\n",
    "b_i & \\text{if } v_i^k > b_i.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "**The dual problem** looks like\n",
    "$$\n",
    "\\underset{\\mu \\in \\mathbb{R}^{n}}{\\text{minimize}}\\; f^\\ast(-\\mu) + \\iota^\\ast_{S}(\\mu)\n",
    "$$\n",
    "implying that the proximal gradient update becomes\n",
    "$$\n",
    "    \\mu^{k+1} = \\prox_{\\gamma \\iota^\\ast}\\left(\\mu^k - \\gamma \\nabla f(-\\mu^k)\\right) = \\prox_{\\gamma \\iota^\\ast}\\left(\\mu^k + \\gamma \\left(Q^{-1}\\left(-\\mu^k-q\\right)\\right)\\right) =  \\prox_{\\gamma \\iota^\\ast}\\left( \\mu^k - \\gamma \\left(Q^{-1}\\left(\\mu^k+q\\right)\\right)\\right).\n",
    "$$\n",
    "Similarly, we can let  $u^k =\\mu^k - \\gamma \\left(Q^{-1}\\left(\\mu^k+q\\right)\\right)$ leading to\n",
    "$$\n",
    "\\mu^{k+1} = \\prox_{\\gamma \\iota^\\ast}\\left(u^k\\right),\n",
    "$$\n",
    "and again from the previous task we know \n",
    "$$\n",
    "\\mu^{k+1} = \\prox_{\\gamma \\iota^\\ast}\\left(u^k\\right) = [x_1^k,..., x_n^k]^\\top,\n",
    "$$\n",
    "where $x_i$ is now instead given by\n",
    "$$\n",
    "x_i^k=\\begin{cases} \n",
    "u_i^k-\\gamma a_i & \\text{if } \\gamma^{-1}u_i^k < a_i, \\\\\n",
    "0 & \\text{if } a_i \\leq \\gamma^{-1}u_i^k \\leq b_i,\\\\\n",
    "u_i^k - \\gamma b_i & \\text{if } \\gamma^{-1}u_i^k > b_i.\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\DeclareMathOperator{\\prox}{prox}$\n",
    "$\\newcommand{\\R}{\\mathbb{R}}$\n",
    "$\\newcommand{\\Sym}{\\mathbb{S}}$\n",
    "$\\newcommand{\\norm}[1]{\\lVert{#1}\\rVert}$\n",
    "$\\DeclareMathOperator{\\sign}{sign}$\n",
    "$\\DeclareMathOperator{\\diag}{diag}$\n",
    "$\\DeclareMathOperator{\\relint}{relint}$\n",
    "$\\DeclareMathOperator{\\dom}{dom}$\n",
    "---\n",
    "### Task 7:\n",
    "\n",
    "Suppose that $\\mu^\\star\\in\\R^{n}$ is an optimal solution to the dual problem you found in Task 3. Given $\\mu^\\star$, and __starting from the optimality condition for the dual problem (given by _Fermat's rule_)__, recover an optimal point $x^{\\star}\\in\\R^{n}$ to the primal problem (2), and show that this $x^{\\star}$ is in fact an optimal solution to the primal problem (2). \n",
    "\n",
    "__Solution:__ \n",
    "\n",
    "Our dual problem is\n",
    "$$\n",
    "\\underset{\\mu \\in \\mathbb{R}^{n}}{\\text{minimize}}\\; (f^\\ast)(-L^T\\mu) + \\iota^\\ast_{S}(\\mu),\n",
    "$$\n",
    "where $L = I$. In this case the dual optimality condition is given by:\n",
    "$$\n",
    "0 \\in -L\\partial f^\\ast(-L^T\\mu^\\ast) + \\partial \\iota^\\ast_{S}(\\mu^\\ast) = -\\partial f^\\ast(-\\mu^\\ast) + \\partial \\iota^\\ast_{S}(\\mu^\\ast).\n",
    "$$\n",
    "As seen in class we have that the optimal primal $x^\\ast$ must satisfy \n",
    "$$\n",
    "x^\\ast \\in \\partial f^\\ast(-L^T \\mu^\\ast)= \\partial (f^\\ast\\circ(-I))(\\mu^\\ast) = \\{\\nabla f^\\ast(-\\mu^\\ast)\\} = \\{-Q^{-1}(-\\mu^\\ast-q)\\} = \\{Q^{-1}(\\mu^\\ast+q)\\},\n",
    "$$\n",
    "since $f^\\ast$ is differentiable, and because this is a signleton set we have\n",
    "$$\n",
    "x^\\ast = Q^{-1}(\\mu^\\ast+q).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\DeclareMathOperator{\\prox}{prox}$\n",
    "$\\newcommand{\\R}{\\mathbb{R}}$\n",
    "$\\newcommand{\\Sym}{\\mathbb{S}}$\n",
    "$\\newcommand{\\norm}[1]{\\lVert{#1}\\rVert}$\n",
    "$\\DeclareMathOperator{\\sign}{sign}$\n",
    "$\\DeclareMathOperator{\\diag}{diag}$\n",
    "$\\DeclareMathOperator{\\relint}{relint}$\n",
    "$\\DeclareMathOperator{\\dom}{dom}$\n",
    "--- \n",
    "### Task 8:\n",
    "\n",
    "Use your results above to fill in the functions below.\n",
    "\n",
    "__Solution:__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def quad(x,Q,q):\n",
    "    \"\"\"\n",
    "    quad(x,Q,q) computes the quadratic function (1/2)x'Qx + q'x\n",
    "    \n",
    "    :param x: the variable of the quadratic function\n",
    "    :param Q: the matrix in the quadratic function that corresponds to the quadratic form\n",
    "    :param q: the vector in the quadratic function that corresponds to the linear part\n",
    "    :return: (1/2)x'Qx + q'x\n",
    "    \"\"\"\n",
    "    # Write your solution here\n",
    "    return ...\n",
    "\n",
    "def quadconj(mu,Q,q):\n",
    "    \"\"\"\n",
    "    quadconj(mu,Q,q) computes the conjugate function of the \n",
    "    quadratic function (1/2)x'Qx + q'x, evaluated at mu\n",
    "    \n",
    "    :param mu: the variable of the conjugate function\n",
    "    :param Q: the matrix in the quadratic function that corresponds to the quadratic form\n",
    "    :param q: the vector in the quadratic function that corresponds to the linear part\n",
    "    :return: conjugate of (1/2)x'Qx + q'x, evaluated at mu\n",
    "    \"\"\"\n",
    "    # Write your solution here\n",
    "    return ...\n",
    "\n",
    "def box(x,a,b):\n",
    "    \"\"\"\n",
    "    box(x,a,b) computes the indicator function of the box contraint\n",
    "    [a,b]\n",
    "    \n",
    "    :param x: the variable of the indicator function\n",
    "    :param a: the left vector defining the box contraint\n",
    "    :param b: the right vector defining the box contraint\n",
    "    :return: 0 if x is in [a,b] and infinity otherwise\n",
    "    \"\"\"\n",
    "    if np.all(a <= x) and np.all(x <= b):\n",
    "        return 0\n",
    "    else: \n",
    "        return np.Inf\n",
    "\n",
    "def boxconj(mu,a,b):\n",
    "    \"\"\"\n",
    "    boxconj(mu,a,b) computes the conjugate function of the indicator function \n",
    "    of the box contraint [a,b], evaluated at mu\n",
    "    \n",
    "    :param mu: the variable of the conjugate function\n",
    "    :param a: the left vector defining the box contraint\n",
    "    :param b: the right vector defining the box contraint\n",
    "    :return: conjugate of the indicator function of the box contraint [a,b], evaluated at mu\n",
    "    \"\"\"\n",
    "    # Write your solution here\n",
    "    return ...\n",
    "\n",
    "def grad_quad(x,Q,q):\n",
    "    \"\"\"\n",
    "    grad_quad(x,Q,q) computes the gradient of the quadratic function (1/2)x'Qx + q'x\n",
    "    \n",
    "    :param x: the variable of the quadratic function\n",
    "    :param Q: the matrix in the quadratic function that corresponds to the quadratic form\n",
    "    :param q: the vector in the quadratic function that corresponds to the linear part\n",
    "    :return: gradient of (1/2)x'Qx + q'x\n",
    "    \"\"\"\n",
    "    # Write your solution here\n",
    "    return ...\n",
    "\n",
    "def grad_quadconj(mu,Q,q):\n",
    "    \"\"\"\n",
    "    grad_quadconj(mu,Q,q) computes the gradient of the conjugate function of the \n",
    "    the quadratic function (1/2)x'Qx + q'x, evaluated at mu\n",
    "    \n",
    "    :param mu: the variable of the conjugate function\n",
    "    :param Q: the matrix in the quadratic function that corresponds to the quadratic form\n",
    "    :param q: the vector in the quadratic function that corresponds to the linear part\n",
    "    :return: gradient of the conjugate of (1/2)x'Qx + q'x, evaluated at mu\n",
    "    \"\"\"\n",
    "    # Write your solution here\n",
    "    return ...\n",
    "\n",
    "def prox_box(x,a,b,gamma):\n",
    "    \"\"\"\n",
    "    prox_box(x,a,b,gamma) computes proximal operator of the indicator function \n",
    "    of the box contraint [a,b], evaluated at x\n",
    "    \n",
    "    :param x: the variable of the poximal operator\n",
    "    :param a: the left vector defining the box contraint\n",
    "    :param b: the right vector defining the box contraint\n",
    "    :param gamma: the step-size parameter\n",
    "    :return: proximal operator of the indicator function of the \n",
    "    box contraint [a,b], evaluated at x\n",
    "    \"\"\"\n",
    "    # Write your solution here\n",
    "    return ...\n",
    "\n",
    "def prox_boxconj(mu,a,b,gamma):\n",
    "    \"\"\"\n",
    "    prox_box(mu,a,b,gamma) computes proximal operator of the conjugate function of \n",
    "    the indicator function of the box contraint [a,b], evaluated at mu\n",
    "    \n",
    "    :param mu: the variable of the poximal operator\n",
    "    :param a: the left vector defining the box contraint\n",
    "    :param b: the right vector defining the box contraint\n",
    "    :param gamma: the step-size parameter\n",
    "    :return: proximal operator of the conjugate function of the indicator function of the \n",
    "    box contraint [a,b], evaluated at mu\n",
    "    \"\"\"\n",
    "    # Write your solution here\n",
    "    return ...\n",
    "\n",
    "def dual_to_primal(mu,Q,q,a,b):\n",
    "    \"\"\"\n",
    "    dual_to_primal(mu,Q,q,a,b) computes the solution x* to the primal problem \n",
    "    given a solution mu* to the dual problem.\n",
    "    \n",
    "    :param mu: the dual variable\n",
    "    :param Q: the matrix in the quadratic function that corresponds to the quadratic form\n",
    "    :param q: the vector in the quadratic function that corresponds to the linear part\n",
    "    :param a: the left vector defining the box contraint\n",
    "    :param b: the right vector defining the box contraint\n",
    "    :return: the extracted primal variable\n",
    "    \"\"\"\n",
    "    # Write your solution here\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\DeclareMathOperator{\\prox}{prox}$\n",
    "$\\newcommand{\\R}{\\mathbb{R}}$\n",
    "$\\newcommand{\\Sym}{\\mathbb{S}}$\n",
    "$\\newcommand{\\norm}[1]{\\lVert{#1}\\rVert}$\n",
    "$\\DeclareMathOperator{\\sign}{sign}$\n",
    "$\\DeclareMathOperator{\\diag}{diag}$\n",
    "$\\DeclareMathOperator{\\relint}{relint}$\n",
    "$\\DeclareMathOperator{\\dom}{dom}$\n",
    "---\n",
    "### Task 9:\n",
    "\n",
    "Below is a function for generating $Q$, $q$, $a$, and $b$ that define the quadratic function $f$ and the box constraint set $S$. Use Task 8 to solve the primal problem using the proximal gradient method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def problem_data():\n",
    "    \"\"\"\n",
    "    problem_data() generates the problem data variables Q, q, a and b\n",
    "    \n",
    "    :return: (Q,q,a,b)\n",
    "    \"\"\"\n",
    "    rs = np.random.RandomState(np.random.MT19937(np.random.SeedSequence(1)))\n",
    "    n = 20\n",
    "    Q = rs.randn(n,n)\n",
    "    Q = Q.T@Q\n",
    "    q = rs.randn(n)\n",
    "    a = -rs.rand(n)\n",
    "    b = rs.rand(n)\n",
    "    return Q, q, a, b\n",
    "\n",
    "(Q,q,a,b) = problem_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__a)__ What seems to be the best choice of $\\gamma$? \n",
    "\n",
    "__Solution:__ \n",
    "\n",
    "_Fill in your solution here!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\DeclareMathOperator{\\prox}{prox}$\n",
    "$\\newcommand{\\R}{\\mathbb{R}}$\n",
    "$\\newcommand{\\Sym}{\\mathbb{S}}$\n",
    "$\\newcommand{\\norm}[1]{\\lVert{#1}\\rVert}$\n",
    "$\\DeclareMathOperator{\\sign}{sign}$\n",
    "$\\DeclareMathOperator{\\diag}{diag}$\n",
    "$\\DeclareMathOperator{\\relint}{relint}$\n",
    "$\\DeclareMathOperator{\\dom}{dom}$\n",
    "\n",
    "__b)__ Does the upper bound $\\gamma < \\frac{2}{\\beta}$ seem reasonable?\n",
    "\n",
    "__Solution:__ \n",
    "\n",
    "_Fill in your solution here!_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\DeclareMathOperator{\\prox}{prox}$\n",
    "$\\newcommand{\\R}{\\mathbb{R}}$\n",
    "$\\newcommand{\\Sym}{\\mathbb{S}}$\n",
    "$\\newcommand{\\norm}[1]{\\lVert{#1}\\rVert}$\n",
    "$\\DeclareMathOperator{\\sign}{sign}$\n",
    "$\\DeclareMathOperator{\\diag}{diag}$\n",
    "$\\DeclareMathOperator{\\relint}{relint}$\n",
    "$\\DeclareMathOperator{\\dom}{dom}$\n",
    "\n",
    "Test different initial points for the algorithm:\n",
    "\n",
    "__c)__ Does this affect the point the algorithm converges to? \n",
    "\n",
    "__Solution:__ \n",
    "\n",
    "_Fill in your solution here!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\DeclareMathOperator{\\prox}{prox}$\n",
    "$\\newcommand{\\R}{\\mathbb{R}}$\n",
    "$\\newcommand{\\Sym}{\\mathbb{S}}$\n",
    "$\\newcommand{\\norm}[1]{\\lVert{#1}\\rVert}$\n",
    "$\\DeclareMathOperator{\\sign}{sign}$\n",
    "$\\DeclareMathOperator{\\diag}{diag}$\n",
    "$\\DeclareMathOperator{\\relint}{relint}$\n",
    "$\\DeclareMathOperator{\\dom}{dom}$\n",
    "\n",
    "__d)__ Carefully motivate theoretically why/why not it affects the final point. _Hint:_ Look at the objective function in (2).\n",
    "\n",
    "__Solution:__ \n",
    "\n",
    "_Fill in your solution here!_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\DeclareMathOperator{\\prox}{prox}$\n",
    "$\\newcommand{\\R}{\\mathbb{R}}$\n",
    "$\\newcommand{\\Sym}{\\mathbb{S}}$\n",
    "$\\newcommand{\\norm}[1]{\\lVert{#1}\\rVert}$\n",
    "$\\DeclareMathOperator{\\sign}{sign}$\n",
    "$\\DeclareMathOperator{\\diag}{diag}$\n",
    "$\\DeclareMathOperator{\\relint}{relint}$\n",
    "$\\DeclareMathOperator{\\dom}{dom}$\n",
    "\n",
    "__e)__ Does your final point $x^{\\text{final}}$ satisfy the constraint $x^{\\text{final}} \\in S$?\n",
    "\n",
    "__Solution:__ \n",
    "\n",
    "_Fill in your solution here!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\DeclareMathOperator{\\prox}{prox}$\n",
    "$\\newcommand{\\R}{\\mathbb{R}}$\n",
    "$\\newcommand{\\Sym}{\\mathbb{S}}$\n",
    "$\\newcommand{\\norm}[1]{\\lVert{#1}\\rVert}$\n",
    "$\\DeclareMathOperator{\\sign}{sign}$\n",
    "$\\DeclareMathOperator{\\diag}{diag}$\n",
    "$\\DeclareMathOperator{\\relint}{relint}$\n",
    "$\\DeclareMathOperator{\\dom}{dom}$\n",
    "\n",
    "__f)__ What about the iterates, do they always satisfy the constraint, $x^k \\in S$? Why/why not?\n",
    "\n",
    "__Solution:__ \n",
    "\n",
    "_Fill in your solution here!_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\DeclareMathOperator{\\prox}{prox}$\n",
    "$\\newcommand{\\R}{\\mathbb{R}}$\n",
    "$\\newcommand{\\Sym}{\\mathbb{S}}$\n",
    "$\\newcommand{\\norm}[1]{\\lVert{#1}\\rVert}$\n",
    "$\\DeclareMathOperator{\\sign}{sign}$\n",
    "$\\DeclareMathOperator{\\diag}{diag}$\n",
    "$\\DeclareMathOperator{\\relint}{relint}$\n",
    "$\\DeclareMathOperator{\\dom}{dom}$\n",
    "---\n",
    "### Task 10:\n",
    "\n",
    "Solve the dual problem. \n",
    "\n",
    "__a)__ Similar to the previous task, find/verify the upper bound on the step-size and find a good step-size choice.\n",
    "\n",
    "__Solution:__ \n",
    "\n",
    "_Fill in your solution here!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\DeclareMathOperator{\\prox}{prox}$\n",
    "$\\newcommand{\\R}{\\mathbb{R}}$\n",
    "$\\newcommand{\\Sym}{\\mathbb{S}}$\n",
    "$\\newcommand{\\norm}[1]{\\lVert{#1}\\rVert}$\n",
    "$\\DeclareMathOperator{\\sign}{sign}$\n",
    "$\\DeclareMathOperator{\\diag}{diag}$\n",
    "$\\DeclareMathOperator{\\relint}{relint}$\n",
    "$\\DeclareMathOperator{\\dom}{dom}$\n",
    "\n",
    "Let $x^{\\text{final}}$ be the final points from Task 9 and $\\mu^{\\text{final}}$ the final point for the dual problem. Let $\\hat{x}^{\\text{final}}$ final primal points extracted from the final dual point $\\mu^{\\text{final}}$ using the expression from Task 7:\n",
    "\n",
    "__b)__ Are $x^{\\text{final}}$ and $\\hat{x}^{\\text{final}}$ the same?\n",
    "\n",
    "__Solution:__ \n",
    "\n",
    "_Fill in your solution here!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\DeclareMathOperator{\\prox}{prox}$\n",
    "$\\newcommand{\\R}{\\mathbb{R}}$\n",
    "$\\newcommand{\\Sym}{\\mathbb{S}}$\n",
    "$\\newcommand{\\norm}[1]{\\lVert{#1}\\rVert}$\n",
    "$\\DeclareMathOperator{\\sign}{sign}$\n",
    "$\\DeclareMathOperator{\\diag}{diag}$\n",
    "$\\DeclareMathOperator{\\relint}{relint}$\n",
    "$\\DeclareMathOperator{\\dom}{dom}$\n",
    "\n",
    "__c)__ Is $\\hat{x}^{\\text{final}}$ in the box $S$?\n",
    "\n",
    "__Solution:__ \n",
    "\n",
    "_Fill in your solution here!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\DeclareMathOperator{\\prox}{prox}$\n",
    "$\\newcommand{\\R}{\\mathbb{R}}$\n",
    "$\\newcommand{\\Sym}{\\mathbb{S}}$\n",
    "$\\newcommand{\\norm}[1]{\\lVert{#1}\\rVert}$\n",
    "$\\DeclareMathOperator{\\sign}{sign}$\n",
    "$\\DeclareMathOperator{\\diag}{diag}$\n",
    "$\\DeclareMathOperator{\\relint}{relint}$\n",
    "$\\DeclareMathOperator{\\dom}{dom}$\n",
    "\n",
    "__d)__ Let $\\mu^k$ be the iterates of the dual method, using the expression from Task 7, extract the primal iterates $\\hat{x}^k$ from $\\mu^k$. Does $\\hat{x}^k$ always satisfy the constraint $\\hat{x}^k \\in S$?\n",
    "\n",
    "__Solution:__ \n",
    "\n",
    "_Fill in your solution here!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\DeclareMathOperator{\\prox}{prox}$\n",
    "$\\newcommand{\\R}{\\mathbb{R}}$\n",
    "$\\newcommand{\\Sym}{\\mathbb{S}}$\n",
    "$\\newcommand{\\norm}[1]{\\lVert{#1}\\rVert}$\n",
    "$\\DeclareMathOperator{\\sign}{sign}$\n",
    "$\\DeclareMathOperator{\\diag}{diag}$\n",
    "$\\DeclareMathOperator{\\relint}{relint}$\n",
    "$\\DeclareMathOperator{\\dom}{dom}$\n",
    "\n",
    "__e)__ How do the function values $f\\left(\\hat{x}^k\\right)$ develop over the iterations?\n",
    "\n",
    "__Solution:__ \n",
    "\n",
    "_Fill in your solution here!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\DeclareMathOperator{\\prox}{prox}$\n",
    "$\\newcommand{\\R}{\\mathbb{R}}$\n",
    "$\\newcommand{\\Sym}{\\mathbb{S}}$\n",
    "$\\newcommand{\\norm}[1]{\\lVert{#1}\\rVert}$\n",
    "$\\DeclareMathOperator{\\sign}{sign}$\n",
    "$\\DeclareMathOperator{\\diag}{diag}$\n",
    "$\\DeclareMathOperator{\\relint}{relint}$\n",
    "$\\DeclareMathOperator{\\dom}{dom}$\n",
    "\n",
    "__f)__ What about $f\\left(\\hat{x}^k\\right)+\\iota_{S}\\left(\\hat{x}^k\\right)$?\n",
    "\n",
    "__Solution:__ \n",
    "\n",
    "_Fill in your solution here!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your solution here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
